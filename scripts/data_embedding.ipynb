{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cifar_data import load_cifar_data\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On charge nos données brutes du dataset CIFAR-10\n",
    "X, y = load_cifar_data('../data/cifar')\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/axellabrousse/Documents/3A/Cours Lyon1/ML/ml_project/env/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/axellabrousse/Documents/3A/Cours Lyon1/ML/ml_project/env/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Redimensionner les images CIFAR-10 en 3x32x32 pour correspondre à l'entrée de ResNet18\n",
    "X = X.view(-1, 3, 32, 32)  # Reshape les images CIFAR-10 de (3072,) à (3, 32, 32)\n",
    "\n",
    "# Normalisation des données CIFAR-10 en utilisant les valeurs moyennes et les écarts-types standard\n",
    "transform = transforms.Compose([\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Appliquer la transformation de normalisation\n",
    "X = transform(X)\n",
    "\n",
    "# Charger le modèle pré-entraîné ResNet18\n",
    "resnet18 = torchvision.models.resnet18(pretrained=True)\n",
    "\n",
    "# Remplacer la couche fully connected finale pour qu'elle ne change pas la dimensionnalité\n",
    "# de sortie pour nos embeddings. Nous voulons les features avant la classification finale.\n",
    "resnet18 = nn.Sequential(*list(resnet18.children())[:-1])\n",
    "\n",
    "# Passer les images à travers ResNet18 pour obtenir les embeddings\n",
    "def get_embeddings(X):\n",
    "    # Assurez-vous que le modèle est en mode évaluation\n",
    "    resnet18.eval()\n",
    "\n",
    "    embeddings_list = []\n",
    "    # Utiliser tqdm pour afficher la barre de progression\n",
    "    for i in tqdm(range(X.size(0)), desc=\"Processing Images\", unit=\"image\"):\n",
    "        image = X[i].unsqueeze(0)  # Ajouter une dimension pour simuler un batch\n",
    "        with torch.no_grad():\n",
    "            embedding = resnet18(image)\n",
    "            embedding = embedding.view(embedding.size(0), -1)  # Redimensionner les embeddings\n",
    "            embeddings_list.append(embedding)\n",
    "    \n",
    "    # Concaténer les embeddings obtenus pour toutes les images\n",
    "    embeddings = torch.cat(embeddings_list, dim=0).numpy()\n",
    "    \n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Images: 100%|██████████| 60000/60000 [02:23<00:00, 419.36image/s]\n"
     ]
    }
   ],
   "source": [
    "# `embeddings` contiendra maintenant les représentations d'images de dimension (N, 512)\n",
    "embeddings = get_embeddings(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialiser le scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Appliquer la normalisation (scaling) sur les embeddings\n",
    "embeddings_scaled = scaler.fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_df = pd.DataFrame(embeddings)\n",
    "y_np = y.numpy()\n",
    "embeddings_df['target'] = y_np\n",
    "\n",
    "# Enregistrer le DataFrame dans un fichier CSV\n",
    "embeddings_df.to_csv('../data/ref_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On enregistre le modèle ResNet de l'embedding et le scaler\n",
    "with open(\"../artifacts/resnet18_embedding.pkl\", \"wb\") as f:\n",
    "    pickle.dump(resnet18, f)\n",
    "\n",
    "with open('../artifacts/scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_resnet50_embeddings():\n",
    "    transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize for ResNet50\n",
    "    transforms.ToTensor(),          # Convert to tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize for ResNet\n",
    "])\n",
    "\n",
    "    # Load train and test datasets\n",
    "    train_dataset = torchvision.datasets.CIFAR10(root='../data/cifar/', train=True, download=True, transform=transform)\n",
    "    test_dataset = torchvision.datasets.CIFAR10(root='../data/cifar', train=False, download=True, transform=transform)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=False)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "    \n",
    "    # Step 2: Load pretrained ResNet50\n",
    "    device = torch.device(\"mps\" if torch.mps.is_available() else \"cpu\")\n",
    "    resnet = torchvision.models.resnet50(pretrained=True).to(device)\n",
    "    resnet.fc = nn.Identity()  # Remove the classification head\n",
    "    \n",
    "    # Function to extract embeddings\n",
    "    def extract_embeddings(dataloader, model, device):\n",
    "        model.eval()\n",
    "        embeddings = []\n",
    "        labels = []\n",
    "        with torch.no_grad():\n",
    "            for images, targets in tqdm(dataloader):\n",
    "                images = images.to(device)\n",
    "                features = model(images)  # Extract features\n",
    "                embeddings.append(features.cpu().numpy())\n",
    "                labels.append(targets.numpy())\n",
    "        embeddings = np.concatenate(embeddings, axis=0)\n",
    "        labels = np.concatenate(labels, axis=0)\n",
    "        return embeddings, labels\n",
    "    \n",
    "    # Extract embeddings for training and test datasets\n",
    "    train_embeddings, train_labels = extract_embeddings(train_loader, resnet, device)\n",
    "    test_embeddings, test_labels = extract_embeddings(test_loader, resnet, device)\n",
    "\n",
    "    # Initialiser le scaler\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Appliquer la normalisation (scaling) sur les embeddings\n",
    "    # Concatenate train and test embeddings\n",
    "    all_embeddings = np.concatenate((train_embeddings, test_embeddings), axis=0)\n",
    "    all_labels = np.concatenate((train_labels, test_labels), axis=0)\n",
    "\n",
    "    # Apply scaling\n",
    "    embeddings_scaled = scaler.fit_transform(all_embeddings)\n",
    "\n",
    "    # Create DataFrame\n",
    "    embeddings_df = pd.DataFrame(embeddings_scaled)\n",
    "    embeddings_df['target'] = all_labels\n",
    "    embeddings_df.to_csv('../data/ref_data.csv', index=False)\n",
    "\n",
    "    resnet.to('cpu')\n",
    "    # On enregistre le modèle ResNet de l'embedding et le scaler\n",
    "    with open(\"../artifacts/resnet50_embedding.pkl\", \"wb\") as f:\n",
    "        pickle.dump(resnet, f)\n",
    "\n",
    "    with open('../artifacts/scaler_resnet50.pkl', 'wb') as f:\n",
    "        pickle.dump(scaler, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/axellabrousse/Documents/3A/Cours Lyon1/ML/ml_project/env/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/axellabrousse/Documents/3A/Cours Lyon1/ML/ml_project/env/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "100%|██████████| 782/782 [08:27<00:00,  1.54it/s]\n",
      "100%|██████████| 157/157 [01:50<00:00,  1.42it/s]\n"
     ]
    }
   ],
   "source": [
    "get_resnet50_embeddings()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
